{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04905349",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56315e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: twython in c:\\users\\prana\\anaconda3\\lib\\site-packages (3.9.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: requests>=2.1.0 in c:\\users\\prana\\anaconda3\\lib\\site-packages (from twython) (2.31.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.4.0 in c:\\users\\prana\\anaconda3\\lib\\site-packages (from twython) (2.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\prana\\anaconda3\\lib\\site-packages (from requests>=2.1.0->twython) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\prana\\anaconda3\\lib\\site-packages (from requests>=2.1.0->twython) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\prana\\anaconda3\\lib\\site-packages (from requests>=2.1.0->twython) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\prana\\anaconda3\\lib\\site-packages (from requests>=2.1.0->twython) (2023.7.22)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\prana\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.4.0->twython) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "pip install twython "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d7d774a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vaderSentiment in c:\\users\\prana\\anaconda3\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: requests in c:\\users\\prana\\anaconda3\\lib\\site-packages (from vaderSentiment) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\prana\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\prana\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\prana\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\prana\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef5548a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from colorama import Fore, init\n",
    "import plotly.express as px\n",
    "\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk import tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61bfd2d",
   "metadata": {},
   "source": [
    " ## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be82dfe0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'sentimentdataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentimentdataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1662\u001b[0m     f,\n\u001b[0;32m   1663\u001b[0m     mode,\n\u001b[0;32m   1664\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1665\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1666\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1667\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1668\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1669\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1670\u001b[0m )\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    862\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    863\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sentimentdataset.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"sentimentdataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ddc316",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa6e801",
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_count():\n",
    "    return pd.DataFrame({'features': df.columns,\n",
    "                'dtypes': df.dtypes.values,\n",
    "                'NaN count': df.isnull().sum().values,\n",
    "                'NaN percentage': df.isnull().sum().values/df.shape[0]}).style.background_gradient(cmap='Set3',low=0.1,high=0.01)\n",
    "null_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0ba9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18c7213",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b012e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    num_distinct_values = len(df[column].unique())\n",
    "    print(f\"{column}: {num_distinct_values} distinct values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84633ff4",
   "metadata": {},
   "source": [
    "## Feature Enginering\n",
    "Drop Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3e685c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Unnamed: 0.1', 'Unnamed: 0', 'Hashtags','Day', 'Hour','Sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c52c52",
   "metadata": {},
   "source": [
    "## Platform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2411fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Platform'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763aa782",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Platform'] = df['Platform'].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a964272d",
   "metadata": {},
   "source": [
    "## Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac305e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa94cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Country'] = df['Country'].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138a9d0f",
   "metadata": {},
   "source": [
    "## Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b34b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "\n",
    "df['Day_of_Week'] = df['Timestamp'].dt.day_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc9a75c",
   "metadata": {},
   "source": [
    " ## Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ad4bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "month_mapping = {\n",
    "    1: 'Januari',\n",
    "    2: 'Februari',\n",
    "    3: 'Maret',\n",
    "    4: 'April',\n",
    "    5: 'Mei',\n",
    "    6: 'Juni',\n",
    "    7: 'Juli',\n",
    "    8: 'Agustus',\n",
    "    9: 'September',\n",
    "    10: 'Oktober',\n",
    "    11: 'November',\n",
    "    12: 'Desember'\n",
    "}\n",
    "\n",
    "df['Month'] = df['Month'].map(month_mapping)\n",
    "\n",
    "df['Month'] = df['Month'].astype('object')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3162c61b",
   "metadata": {},
   "source": [
    "## Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f7e1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)  \n",
    "    text = \" \".join(text.split())\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    cleaned_tokens = [stemmer.stem(token) for token in tokens if token.lower() not in stop_words]\n",
    "   \n",
    "    cleaned_text = ' '.join(cleaned_tokens)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "df[\"Clean_Text\"] = df[\"Text\"].apply(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99abefc3",
   "metadata": {},
   "source": [
    "## Unique Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebaf137",
   "metadata": {},
   "outputs": [],
   "source": [
    "specified_columns = ['Platform','Country', 'Year','Month','Day_of_Week']\n",
    "\n",
    "for col in specified_columns:\n",
    "    total_unique_values = df[col].nunique()\n",
    "    print(f'Total unique values for {col}: {total_unique_values}')\n",
    "\n",
    "    top_values = df[col].value_counts()\n",
    "\n",
    "    colors = [Fore.RED, Fore.GREEN, Fore.YELLOW, Fore.BLUE, Fore.MAGENTA, Fore.CYAN, Fore.WHITE, Fore.LIGHTBLACK_EX, Fore.LIGHTRED_EX, Fore.LIGHTGREEN_EX]\n",
    "\n",
    "    for i, (value, count) in enumerate(top_values.items()):\n",
    "        color = colors[i % len(colors)]\n",
    "        print(f'{color}{value}: {count}{Fore.RESET}')\n",
    "\n",
    "    print('\\n' + '=' * 30 + '\\n')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5421ff",
   "metadata": {},
   "source": [
    "## E D A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f2e217",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1369890b",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8d0f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "df1['Vader_Score'] = df1['Clean_Text'].apply(lambda text: analyzer.polarity_scores(text)['compound'])\n",
    "\n",
    "df1['Sentiment'] = df1['Vader_Score'].apply(lambda score: 'positive' if score >= 0.05 else ('negative' if score <= -0.05 else 'neutral'))\n",
    "\n",
    "print(df1[['Clean_Text', 'Vader_Score', 'Sentiment']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7011bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#66b3ff', '#99ff99', '#ffcc99']\n",
    "\n",
    "explode = (0.1, 0, 0)  \n",
    "\n",
    "sentiment_counts = df1.groupby(\"Sentiment\").size()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "wedges, texts, autotexts = ax.pie(\n",
    "    x=sentiment_counts, \n",
    "    labels=sentiment_counts.index,\n",
    "    autopct=lambda p: f'{p:.2f}%\\n({int(p*sum(sentiment_counts)/100)})', \n",
    "    wedgeprops=dict(width=0.7),\n",
    "    textprops=dict(size=10, color=\"r\"),  \n",
    "    pctdistance=0.7,\n",
    "    colors=colors,\n",
    "    explode=explode,\n",
    "    shadow=True)\n",
    "\n",
    "center_circle = plt.Circle((0, 0), 0.6, color='white', fc='white', linewidth=1.25)\n",
    "fig.gca().add_artist(center_circle)\n",
    "\n",
    "ax.text(0, 0, 'Sentiment\\nDistribution', ha='center', va='center', fontsize=14, fontweight='bold', color='#333333')\n",
    "\n",
    "ax.legend(sentiment_counts.index, title=\"Sentiment\", loc=\"center left\", bbox_to_anchor=(1, 0, 0.5, 1))\n",
    "\n",
    "ax.axis('equal')  \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d4a6c6",
   "metadata": {},
   "source": [
    "## Year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277a3ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(x='Year', hue='Sentiment', data=df1, palette='Paired')\n",
    "plt.title('Relationship between Years and Sentiment')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f857fe5",
   "metadata": {},
   "source": [
    "## Day Of Weeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61f7e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(x='Day_of_Week', hue='Sentiment', data=df1, palette='Paired')\n",
    "plt.title('Relationship between Day of Week and Sentiment')\n",
    "plt.xlabel('Day of Week')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dd75a6",
   "metadata": {},
   "source": [
    "## Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1659a526",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(x='Month', hue='Sentiment', data=df1, palette='Paired')\n",
    "plt.title('Relationship between Month and Sentiment')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6127ed9a",
   "metadata": {},
   "source": [
    "## Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095a4444",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(x='Platform', hue='Sentiment', data=df1, palette='Paired')\n",
    "plt.title('Relationship between Platform and Sentiment')\n",
    "plt.xlabel('Platform')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f691c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "top_10_countries = df1['Country'].value_counts().head(10).index\n",
    "\n",
    "df_top_10_countries = df1[df1['Country'].isin(top_10_countries)]\n",
    "\n",
    "sns.countplot(x='Country', hue='Sentiment', data=df_top_10_countries, palette='Paired')\n",
    "plt.title('Relationship between Country and Sentiment (Top 10 Countries)')\n",
    "plt.xlabel('Country')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158e68a3",
   "metadata": {},
   "source": [
    "## Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9622aef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['temp_list'] = df1['Clean_Text'].apply(lambda x: str(x).split())\n",
    "top_words = Counter([item for sublist in df1['temp_list'] for item in sublist])\n",
    "top_words_df = pd.DataFrame(top_words.most_common(20), columns=['Common_words', 'count'])\n",
    "\n",
    "top_words_df.style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34993142",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['temp_list'] = df1['Clean_Text'].apply(lambda x: str(x).split())\n",
    "top_words = Counter([item for sublist in df1['temp_list'] for item in sublist])\n",
    "top_words_df = pd.DataFrame(top_words.most_common(20), columns=['Common_words', 'count'])\n",
    "\n",
    "fig = px.bar(top_words_df,\n",
    "            x=\"count\",\n",
    "            y=\"Common_words\",\n",
    "            title='Common Words in Text Data',\n",
    "            orientation='h',\n",
    "            width=700,\n",
    "            height=700,\n",
    "            color='Common_words')\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f613d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Positive_sent = df1[df1['Sentiment'] == 'positive']\n",
    "Negative_sent = df1[df1['Sentiment'] == 'negative']\n",
    "Neutral_sent = df1[df1['Sentiment'] == 'neutral']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7eab96",
   "metadata": {},
   "source": [
    "## Positive Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f96d7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "top = Counter([item for sublist in df1[df1['Sentiment'] == 'positive']['temp_list'] for item in sublist])\n",
    "temp_positive = pd.DataFrame(top.most_common(10), columns=['Common_words', 'count'])\n",
    "temp_positive.style.background_gradient(cmap='Greens')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a448a41",
   "metadata": {},
   "source": [
    "## Neutral Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6892ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "top = Counter([item for sublist in df1[df1['Sentiment'] == 'neutral']['temp_list'] for item in sublist])\n",
    "temp_positive = pd.DataFrame(top.most_common(10), columns=['Common_words', 'count'])\n",
    "temp_positive.style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e63edd7",
   "metadata": {},
   "source": [
    "## Negative Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cecf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "top = Counter([item for sublist in df1[df1['Sentiment'] == 'negative']['temp_list'] for item in sublist])\n",
    "temp_positive = pd.DataFrame(top.most_common(10), columns=['Common_words', 'count'])\n",
    "temp_positive.style.background_gradient(cmap='Reds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd25f3c",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08704312",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9731a216",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40a180f",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667615ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df2['Clean_Text'].values\n",
    "y = df2['Sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7ad5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e253754",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc8f27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75be9d93",
   "metadata": {},
   "source": [
    "## Passive Aggressive Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f17e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pac_classifier = PassiveAggressiveClassifier(max_iter=50, random_state=42)\n",
    "pac_classifier.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec40e695",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pac_classifier.predict(X_test_tfidf)\n",
    "accuracy_test = accuracy_score(y_test, y_pred)\n",
    "classification_rep_test = classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ade9bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test Set Results:\")\n",
    "print(f\"Accuracy: {accuracy_test}\")\n",
    "print(\"Classification Report:\\n\", classification_rep_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9d5db4",
   "metadata": {},
   "source": [
    "## Logistic Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1f30c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_classifier = LogisticRegression(max_iter=50, random_state=42)\n",
    "logistic_classifier.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab32030",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_logistic = logistic_classifier.predict(X_test_tfidf)\n",
    "accuracy_logistic = accuracy_score(y_test, y_pred_logistic)\n",
    "classification_rep_logistic = classification_report(y_test, y_pred_logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60293d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic Regression Results:\")\n",
    "print(f\"Accuracy: {accuracy_logistic}\")\n",
    "print(\"Classification Report:\\n\", classification_rep_logistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae31cdc",
   "metadata": {},
   "source": [
    "## Random Fores Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367607c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_classifier = RandomForestClassifier(random_state=42)\n",
    "random_forest_classifier.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c683d7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf = random_forest_classifier.predict(X_test_tfidf)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "classification_rep_rf = classification_report(y_test, y_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c022e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nRandom Forest Results:\")\n",
    "print(f\"Accuracy: {accuracy_rf}\")\n",
    "print(\"Classification Report:\\n\", classification_rep_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5d073e",
   "metadata": {},
   "source": [
    "## SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560a925c",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier = SVC(random_state=42)\n",
    "svm_classifier.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f0c811",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_svm = svm_classifier.predict(X_test_tfidf)\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "classification_rep_svm = classification_report(y_test, y_pred_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ba7f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Support Vector Machine Results:\")\n",
    "print(f\"Accuracy: {accuracy_svm}\")\n",
    "print(\"Classification Report:\\n\", classification_rep_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3302d454",
   "metadata": {},
   "source": [
    "## Multinomial NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcaa61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562f8c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_nb = nb_classifier.predict(X_test_tfidf)\n",
    "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
    "classification_rep_nb = classification_report(y_test, y_pred_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aff8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMultinomial Naive Bayes Results:\")\n",
    "print(f\"Accuracy: {accuracy_nb}\")\n",
    "print(\"Classification Report:\\n\", classification_rep_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f3ff76",
   "metadata": {},
   "source": [
    "## Best Modeling : Passive Aggressive Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e5c303",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcd57a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    'C': [0.1, 0.5, 1.0],\n",
    "    'fit_intercept': [True, False],\n",
    "    'shuffle': [True, False],\n",
    "    'verbose': [0, 1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479cea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "pac_classifier = PassiveAggressiveClassifier(random_state=42)\n",
    "\n",
    "randomized_search = RandomizedSearchCV(pac_classifier, param_distributions=param_dist, n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
    "randomized_search.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69696115",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_randomized = randomized_search.best_params_\n",
    "best_params_randomized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbe5d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pac_classifier_randomized = PassiveAggressiveClassifier(random_state=42, **best_params_randomized)\n",
    "best_pac_classifier_randomized.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47945642",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_best_pac_randomized = best_pac_classifier_randomized.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a3ebbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_best_pac_randomized = accuracy_score(y_test, y_pred_best_pac_randomized)\n",
    "classification_rep_best_pac_randomized = classification_report(y_test, y_pred_best_pac_randomized)\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred_best_pac_randomized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bddb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best PassiveAggressiveClassifier Model (RandomizedSearchCV):\")\n",
    "print(f\"Best Hyperparameters: {best_params_randomized}\")\n",
    "print(f\"Accuracy: {accuracy_best_pac_randomized}\")\n",
    "print(\"Classification Report:\\n\", classification_rep_best_pac_randomized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352c0517",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_test, annot=True, fmt='d', cmap='Greys', xticklabels=['negative', 'neutral', 'positive'], yticklabels=['negative', 'neutral', 'positive'])\n",
    "plt.title('Confusion Matrix - Hyperparameters')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e30417a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(df, figsize=(10,10))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
